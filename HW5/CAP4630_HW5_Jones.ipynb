{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j21j/CAP4630/blob/main/HW5/CAP4630_HW5_Jones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-rR3bJasIvX"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "## Jia Jones HW 5 CAP 4630\n",
        "\n",
        "In this homework assignment, we will train an AI-based explorer to play a game by reinforcement learing. As domestrated below, in this game, the treasure (denoted by T) is on the right-most and the explorer (denoted by o) will learn to get the treasure by moving left and right. The explorer will be rewarded when it gets the treasure.  After serveral epoches, the explorer will learn how to get the treasure faster and finally it will go to the treasure by moving to right directly. \\\n",
        "\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 80 points, with extra 10 bonus points)** \\\n",
        "\n",
        "Episode 1, Step1: o----T   \\\n",
        "... \\\n",
        "Episode 1, Step6: ---o-T   \\\n",
        "... \\\n",
        "Episode 1, Step10: -o---T \\\n",
        "... \\\n",
        "Episode 1, Step15: ----oT (finished) \\\n",
        "\n",
        "**Task Overview:**\n",
        "- Train the explorer getting the treasure quickly through Q-learning method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA7CH0zasLs_"
      },
      "source": [
        "## 1 Achieve Q-learning method ##\n",
        "### 1.1 Model Preparation **(5 Points)**\n",
        "\n",
        "Import useful packages and prepare hyperpaprameters for Q-learning methods.\n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import pandas and rename it to pd.\n",
        "3. Import the library \"time\"\n",
        "4. Set the parameter as suggested\n",
        "\n",
        "**Hints:**\n",
        "1. For your first trial, you may set as it is\n",
        "2. You may explore other possibilities here when you complete the whole homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wirfzkkEsJW4"
      },
      "outputs": [],
      "source": [
        "#import packages here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# N_STATES = 6   # the width of 1-dim world\n",
        "# ACTIONS = ['left', 'right']     # the available actions to use\n",
        "# EPSILON = 0.9   # the degree of greedy (0＜ε＜1)\n",
        "# ALPHA = 0.1     # learning rate (0＜α≤1)\n",
        "# GAMMA = 0.9    # discount factor (0＜γ＜1)\n",
        "# MAX_EPOCHES = 13   # the max epoches\n",
        "# FRESH_TIME = 0.3    # the interval time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0w5LxS3sRYp"
      },
      "source": [
        "### 1.2 Q table **(10 Points)**\n",
        "\n",
        "Q table is a [states * actions] matrix, which stores Q-value of taking one action in that specific state. For example, the following Q table means in state s3, it is more likely to choose a1 because it's Q-value is 5.31 which is higher than Q-value 2.33 for a0 in s3(refer to Lecture slides 16, page 35).\n",
        "![](https://drive.google.com/uc?export=view&id=1WGh7NYyYw6ccrxbDVdfbJmb_IhBfUyFf)\n",
        "\n",
        "**Tasks:**\n",
        "1. define the build_q_table function\n",
        "2. **Print Out** defined Q-table. The correct print information should be:\n",
        "\n",
        "|     | left | right |\n",
        "|-----|------|-------|\n",
        "| 0   | 0.0  | 0.0   |\n",
        "| 1   | 0.0  | 0.0   |\n",
        "| 2   | 0.0  | 0.0   |\n",
        "| 3   | 0.0  | 0.0   |\n",
        "| 4   | 0.0  | 0.0   |\n",
        "| 5   | 0.0  | 0.0   |\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "**Hints:**\n",
        "1. Using pd.DataFrame to define the Q-table.(https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
        "2. Initialize the Q-table with all zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbdlOJB7sOn7"
      },
      "outputs": [],
      "source": [
        "#define the function here\n",
        "# def build_q_table(n_states, actions):\n",
        "    #add your code here\n",
        "\n",
        "# q_table = build_q_table(N_STATES, ACTIONS)\n",
        "# print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfhlKeGWsT4l"
      },
      "source": [
        "### 1.3 Define action **(15 Points)**\n",
        "\n",
        "In this section, we are going to define how an actor picks the actions. We introduce ε-greedy (In lecture slide 16, page 35). In the initial exploring stage, the explorer knows little about the environment. Therefore, it is better to explore randomly instead of greedy. ε-greedy is the value to control the degree of greedy. It can be changed with time lapsing. In this homework, we set it as fixed value EPSILON = 0.9. You can change it to explore the final effect.\n",
        "\n",
        "**Tasks:**\n",
        "1. define the choose_action function\n",
        "2. **Print Out** sample action. The sampled action should be \"left\" or \"right\".\n",
        "\n",
        "**Hints:**\n",
        "1. You need to define two cases: 1) non-greedy (i.e., random); 2) greedy.\n",
        "2. Non-greedy should occupy (1-ε) senario while greedy should occupy ε senario. In this case, it means Non-greedy occupys 10% senario while greedy occupys 90% senario. (you could implement it by comparing a random number ranging from 0 to 1 with ε. **Numpy provides a function capable of generating a random number from a uniform distribution.**)\n",
        "3. In the non-greedy pattern, the actor should choose the actions randomly.\n",
        "4. In the greedy pattern, the actor should choose the higher Q-value action.\n",
        "5. Don't forget the initial state which means all Q-value are zero and actor cannot choose greedily. You can treat it as non-greedy pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtPWgEtosVho"
      },
      "outputs": [],
      "source": [
        "#define the function here\n",
        "# Given state and Q-table, choose action\n",
        "# def choose_action(state, q_table):\n",
        "#       # pick all actions from this state\n",
        "#     if ... or ...:  # non-greedy or non-explored\n",
        "#         ...\n",
        "#     else:\n",
        "#         ...    # greedy\n",
        "#     return action_name\n",
        "\n",
        "# sample_action = choose_action(0, q_table)\n",
        "# print(sample_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQX7PQ5tsXpt"
      },
      "source": [
        "### 1.4 Interact with the environment **(25 Points)**\n",
        "\n",
        "In this section, we need to give a feedback for our previous action, which means getting reward (R) for next state (S_next) based on current state (S_current) and action (A). In this problem, we get reward R=1 if we move to the treasure T spot, otherwise, we get R=0.\n",
        "\n",
        "**Tasks:**\n",
        "1. define get_env_feedback function\n",
        "**Hints:**\n",
        "1. This function contains two parameters S_current and A(ction), and return S_next and R(eward).\n",
        "2. You need to consider two different senarios: 1) A = right; 2) A = left.\n",
        "3. In the above two senarios, you need to consider the boundary, next state and rewards.\n",
        "4. The update_env function is given to show changes for different steps in different episodes.\n",
        "5. The validation for S_current and Action is shown below.\n",
        "\n",
        "- S_current=0, sample_action = 'right', sample_feedback=(1,0)\n",
        "- S_current=3, sample_action = 'right', sample_feedback=(4,0)\n",
        "- S_current=4, sample_action = 'right', sample_feedback=('terminal', 1)\n",
        "- S_current=0, sample_action = 'left', sample_feedback=(0,0)\n",
        "- S_current=3, sample_action = 'left', sample_feedback=(2,0)\n",
        "- S_current=4, sample_action = 'left', sample_feedback=(3, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr1W3h6ksY7V"
      },
      "outputs": [],
      "source": [
        "#define the function here\n",
        "# def get_env_feedback(S_current, A):\n",
        "#     # This is how agent will interact with the environment\n",
        "#     if A == 'right':    # move right\n",
        "#         ...\n",
        "#     else:   # move left\n",
        "#         ...\n",
        "#     return S_next, R\n",
        "\n",
        "# sample_action = 'left'\n",
        "# S_current = 4\n",
        "# sample_feedback = get_env_feedback(S_current, sample_action)\n",
        "# print(sample_feedback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkBKYpDsbxK"
      },
      "outputs": [],
      "source": [
        "# def update_env(S, episode, step_counter):\n",
        "#     # This is how environment be updated\n",
        "#     env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
        "#     if S == 'terminal':\n",
        "#         interaction = '  Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
        "#         print('{}\\n'.format(interaction), end='')\n",
        "#         time.sleep(2)\n",
        "#     else:\n",
        "#         env_list[S] = 'o'\n",
        "#         interaction = ''.join(env_list)\n",
        "#         print('\\r{}'.format(interaction), end='')\n",
        "#         time.sleep(FRESH_TIME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y10Ihx2lse6y"
      },
      "source": [
        "### 1.5 Start Q-learning with defined functions **(25 Points)**\n",
        "\n",
        "In this section, we are going to utilize all the functions defined above to do q-learning based on the optimal policy.\n",
        "![](https://drive.google.com/uc?export=view&id=10ra6mLlBHlhGNTYWwdGANoa6lC1K_7at)\n",
        "\n",
        "**Tasks**:\n",
        "1. define reinforce_learning function\n",
        "\n",
        "**Hints**:\n",
        "1. You should write this function with loops to keep updating q-table until you get to the reward spot.\n",
        "2. We have two loops, one is for different episodes and another one is for steps\n",
        "3. Whenever we take a step to the reward spot, we should end the loop and start another episode.\n",
        "4. Here is one possible example.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oo-gk710XVXbbeI7AI0uZInrnKtqGqn7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad2qCBwEscK_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_umQ6bsf86"
      },
      "outputs": [],
      "source": [
        "#define the function here\n",
        "# def reinforce_learning():\n",
        "#     # main part of RL loop\n",
        "#     # build Q-table here\n",
        "#     ...\n",
        "#     #start training loop\n",
        "#     for episode in range(MAX_EPOCHES):\n",
        "#         step_counter = 0  #counter for counting steps to reach the treasure\n",
        "#         S_current = ...     #start from S_current\n",
        "#         is_terminated = ...   #flag to conrinue or stop the loop\n",
        "#         update_env(S_current, episode, step_counter)   #update environment\n",
        "#         while not is_terminated:\n",
        "#             ...#choose one action\n",
        "#             ...# take action & get next state and reward\n",
        "#             ...#update Q-table\n",
        "#             if S_next != 'terminal':                   #if the explorer doesn't get to the treasure\n",
        "#                 q_target = ...   # if next state is not terminal, how can we estimate the q value (hit: bellman equation)?\n",
        "#             else:\n",
        "#                 q_target = ...     # if next state is terminal, how can we esimate the q value?\n",
        "#                 is_terminated = ...    # terminate this episode\n",
        "\n",
        "#             q_table.loc[S_current, A] = ...  # update Q-table\n",
        "#             ...  # move to next state\n",
        "\n",
        "#             update_env(S_current, episode, step_counter+1)\n",
        "#             step_counter += 1\n",
        "#     return q_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U520WbcsjPS"
      },
      "outputs": [],
      "source": [
        "#main function to run\n",
        "# if __name__ == \"__main__\":\n",
        "#     q_table = reinforce_learning()\n",
        "#     print('\\r\\nQ-table:\\n')\n",
        "#     print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMOSl7IRsmsW"
      },
      "source": [
        "### Bonus Question: Exploring the Impact of Learning Rate and Discount Factor (**10 Bonus Points**)\n",
        "\n",
        "Dive into the dynamics of reinforcement learning by experimenting with two specific configurations of the learning rate (ALPHA α) and discount factor (GAMMA γ). This focused inquiry will shed light on how different emphases on learning speed versus future reward considerations affect an agent's strategy and efficiency.\n",
        "\n",
        "**Your Experimental Setups:**\n",
        "1. **Low Learning Rate, High Discount Factor** (α = 0.1, γ = 0.9): This setting emphasizes cautious learning with a strong consideration for future rewards.\n",
        "2. **High Learning Rate, Low Discount Factor** (α = 0.9, γ = 0.1): Here, the focus shifts to rapid learning with an emphasis on immediate rewards.\n",
        "\n",
        "---\n",
        "\n",
        "#### Task 1: Plotting Steps to Success Over Episodes\n",
        "\n",
        "**Objective:**  \n",
        "Create a line graph to visualize the number of steps the agent takes to reach the goal across episodes for two different sets of Q-learning parameters.\n",
        "\n",
        "**Instructions:**  \n",
        "1. **Gather Data:** Record the number of steps required for the agent to reach the goal in each episode. Do this for both parameter configurations: α=0.1, γ=0.9 and α=0.9, γ=0.1.\n",
        "2. **Prepare the Chart:**\n",
        "   - Label the x-axis as \"Episodes\" and the y-axis as \"Steps to Reach Goal\".\n",
        "   - Choose a plotting tool (e.g., Excel, Google Sheets, Matplotlib, Seaborn).\n",
        "3. **Plot Lines:**\n",
        "   - Draw a line for each parameter set (α=0.1, γ=0.9 and α=0.9, γ=0.1), using different colors or styles to distinguish them.\n",
        "   - Add a legend to identify the lines according to the parameter settings.\n",
        "\n",
        "**Expected Analysis:**  \n",
        "Discuss how the number of steps to reach the goal changes over episodes for each parameter setting. Consider what this suggests about the efficiency of learning and adaptation strategies. Note differences in learning speed and consistency.\n",
        "\n",
        "---\n",
        "\n",
        "#### Task 2: Analyzing Cumulative Reward Patterns\n",
        "\n",
        "**Objective:**  \n",
        "Construct a line graph to illustrate the cumulative reward the agent accumulates over episodes under two different parameter settings: α=0.1, γ=0.9 and α=0.9, γ=0.1.\n",
        "\n",
        "**Instructions:**  \n",
        "1. **Gather Data:** Calculate the cumulative reward that the agent earns from the start to the success in each episode. Track this for both parameter configurations: α=0.1, γ=0.9 and α=0.9, γ=0.1.\n",
        "2. **Prepare the Chart:**\n",
        "   - Label the x-axis as \"Episodes\" and the y-axis as \"Cumulative Reward\".\n",
        "   - Choose a plotting tool (e.g., Excel, Google Sheets, Matplotlib, Seaborn).\n",
        "3. **Plot Lines:**\n",
        "   - Plot a separate line for each parameter configuration, using distinct colors or line styles.\n",
        "   - Clearly label or add a legend to distinguish between the parameter settings.\n",
        "\n",
        "**Expected Analysis:**  \n",
        "Evaluate the patterns in cumulative rewards over episodes for each set of parameters. Discuss the implications of these patterns for the agent's learning process and its ability to maximize rewards. Highlight any notable differences in reward accumulation and learning outcomes between the two parameter sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y2gTD1MsnHt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}