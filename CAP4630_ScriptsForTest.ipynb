{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j21j/CAP4630/blob/main/CAP4630_ScriptsForTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts to help for test"
      ],
      "metadata": {
        "id": "2ThfrKdcIJKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solve to find the value of the partial derivative\n",
        "\n",
        "### HEADS UP!! Number 8 on the homework should be .266 but this code gives .265 as the answer"
      ],
      "metadata": {
        "id": "-Dx4N0MpKLzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sympy as sp\n",
        "\n",
        "# Helper functions\n",
        "def sigmoid(z):\n",
        "    return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def cross_entropy(y, y_hat):\n",
        "    return ((-y * np.log(y_hat)) - ((1 - y) * np.log(1 - y_hat)))\n",
        "\n",
        "def L(x, y, y_hat):\n",
        "    return (-y*np.log(y_hat))+(-(1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "def partial_derivative(func, var_index, x, y, y_hat, h=1e-5):\n",
        "    # Create a list of the variables\n",
        "    vars = [x, y, y_hat]\n",
        "    vars[var_index] += h\n",
        "    return (func(*vars) - func(x, y, y_hat)) / h\n",
        "\n",
        "# Given values\n",
        "x = 2 # input\n",
        "w_1 = 0.5 # weight from input to hidden layer\n",
        "w_2 = -1.0 # weight from hidden to output layer\n",
        "y = 1 # true label\n",
        "\n",
        "# Question 1\n",
        "z_h = w_1 * x # input linear combination\n",
        "a_h = sigmoid(z_h) # input layer activation\n",
        "print(\"1. Input layer (aka sigmoid) activation (aka a_h): \", round(a_h, 3))\n",
        "\n",
        "# Question 2\n",
        "z_0 = w_2 * a_h # output linear combination\n",
        "y_hat = sigmoid(z_0) # output layer activation\n",
        "print(\"2. Output layer activation(aka model output, aka y_hat): \", round(y_hat, 3))\n",
        "\n",
        "# Question 3\n",
        "loss = cross_entropy(y, y_hat)\n",
        "print(\"3. Loss (aka L): \", round(loss, 3))\n",
        "\n",
        "# Question 4\n",
        "dL_dy_hat = partial_derivative(L, 2, x, y, y_hat) # derivative of loss with respect to output layer activation\n",
        "print(\"4. Derivative of loss with respect to predicted output y_hat (aka dL/dy_hat): \", round(dL_dy_hat, 3))\n",
        "\n",
        "# Question 5\n",
        "dy_hat_dz_0 = y_hat * (1 - y_hat) # derivative of output layer activation with respect to output layer linear combination\n",
        "print(\"5. Derivative of output layer activation with respect to output layer linear combination (aka dy_hat/dz_0): \", round(dy_hat_dz_0, 3))\n",
        "\n",
        "# Question 6\n",
        "dz_0_dw_2 = a_h # derivative of output layer linear combination with respect to output layer weight\n",
        "print(\"6. Derivative of output layer linear combination with respect to output layer weight (aka dz_0/dw_2, aka a_h): \", round(dz_0_dw_2, 3))\n",
        "\n",
        "# Question 7\n",
        "dL_dw_2 = dL_dy_hat * dy_hat_dz_0 * dz_0_dw_2 # derivative of loss with respect to output layer weight\n",
        "print(\"7. Derivative of loss with respect to output layer weight 2 (multiply 4, 5, and 6 to get dL/dw_2): \", round(dL_dw_2, 3))\n",
        "\n",
        "# Question 8\n",
        "dL_dz_0 = sigmoid_derivative(z_h) # derivative of loss with respect to output layer linear combination\n",
        "#dL_da_h = dL_dz_0 * w_2 # derivative of loss with respect to hidden layer activation a_h\n",
        "dL_da_h = dy_hat_dz_0 * dL_dy_hat * w_2\n",
        "print(\"8. Derivative of loss with respect to hidden layer activation (aka dL/da_h): \", round(dL_da_h, 3))\n",
        "\n",
        "# Question 9\n",
        "da_h_dz_h = a_h * (1 - a_h) # derivative of hidden layer activation with respect to hidden layer linear combination\n",
        "print(\"9. Derivative of hidden layer activation with respect to hidden layer linear combination (aka da_h/dz_h): \", round(da_h_dz_h, 3))\n",
        "\n",
        "# Question 10\n",
        "dz_h_dw_1 = x # derivative of hidden layer linear combination with respect to hidden layer weight\n",
        "print(\"10. Derivative of hidden layer linear combination with respect to hidden layer weight (aka dz_h/dw_1): \", round(dz_h_dw_1, 3))\n",
        "\n",
        "# Question 11\n",
        "# should be .266 but this code gives .265 as the answer\n",
        "dL_dw_1 = dL_da_h * da_h_dz_h * dz_h_dw_1 # derivative of loss with respect to hidden layer weight\n",
        "print(\"11. Derivative of loss with respect to hidden layer weight 1 (dL/dw_1): \", round(dL_dw_1, 3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfE1Wx7Am68D",
        "outputId": "c15a0f47-3465-4b3f-fa83-252275d0c672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Input layer activation:  0.731\n",
            "2. Output layer activation:  0.325\n",
            "3. Loss:  1.124\n",
            "4. Derivative of loss with respect to output layer activation:  -3.077\n",
            "5. Derivative of output layer activation with respect to output layer linear combination:  0.219\n",
            "6. Derivative of output layer linear combination with respect to output layer weight:  0.731\n",
            "7. Derivative of loss with respect to output layer weight 2:  -0.493\n",
            "8. Derivative of loss with respect to hidden layer activation:  0.675\n",
            "9. Derivative of hidden layer activation with respect to hidden layer linear combination:  0.197\n",
            "10. Derivative of hidden layer linear combination with respect to hidden layer weight:  2\n",
            "11. Derivative of loss with respect to hidden layer weight 1:  0.265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Partial Derivative Equation\n",
        "Used for number 4 and 8"
      ],
      "metadata": {
        "id": "IqGXSR54owZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "# partial derivative of L wrt y_hat\n",
        "L, y, y_hat = sp.symbols('L y y_hat')\n",
        "L = ((-y*sp.log(y_hat)))+(-(1-y)*sp.log(1-y_hat))\n",
        "dL_dy_hat = sp.diff(L, y_hat)\n",
        "print(\"#4 derivative\", dL_dy_hat)\n",
        "\n",
        "#for number 8: partial derivative of L wrt z\n",
        "L, y, z = sp.symbols('L y z')\n",
        "L = (-(y*sp.log(1/(1+2.718281828**-z))))+(-(1-y)*sp.log(1-(1/(1+2.718281828**-z))))\n",
        "dL_dz = sp.diff(L, z)\n",
        "print(\"#8 derivative\", dL_dz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYR5OWF_ovhi",
        "outputId": "eaeb12f7-2cb7-4241-9584-f42eef236b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#4 derivative -y/y_hat - (y - 1)/(1 - y_hat)\n",
            "#8 derivative -0.999999999831127*y/(2.718281828**z*(1 + 2.718281828**(-z))) - 0.999999999831127*(y - 1)/(2.718281828**z*(1 + 2.718281828**(-z))**2*(1 - 1/(1 + 2.718281828**(-z))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Partial Derivative Value\n",
        "Used for number 4 and 8"
      ],
      "metadata": {
        "id": "PFa7in66o00q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def L(x, y, y_hat):\n",
        "    return (-y*np.log(y_hat))+(-(1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "def partial_derivative(func, var_index, x, y, y_hat, h=1e-5):\n",
        "    # Create a list of the variables\n",
        "    vars = [x, y, y_hat]\n",
        "    vars[var_index] += h\n",
        "    return (func(*vars) - func(x, y, y_hat)) / h\n",
        "\n",
        "# Example point\n",
        "x = 1\n",
        "y = 1\n",
        "y_hat = 0.325\n",
        "\n",
        "# Calculate partial derivatives\n",
        "dL_dy_hat = partial_derivative(L, 2, x, y, y_hat)\n",
        "dL_dy = partial_derivative(L, 1, x, y, y_hat)\n",
        "\n",
        "print(\"∂L/∂z =\", dL_dy_hat)\n",
        "print(\"∂L/∂y =\", dL_dy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Yk4dEhI3yj",
        "outputId": "bb0fdb73-04a8-4f04-b1a1-8bc692359ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "∂L/∂z = -3.0768757406152143\n",
            "∂L/∂y = 0.7308875085376697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "k2EZGsjXlyUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class conf_matrix:\n",
        "  def __init__(self, TP, TN, FP, FN):\n",
        "    self.TP = TP\n",
        "    self.TN = TN\n",
        "    self.FP = FP\n",
        "    self.FN = FN\n",
        "    self.accuracy = (TN+TP)/(TN+TP+FP+FN)\n",
        "    self.precision = TP/(TP+FP)\n",
        "    self.recall = TP/(TP+FN)\n",
        "\n",
        "  def print_accuracy(self):\n",
        "    print('Accuracy:', self.accuracy)\n",
        "\n",
        "  def print_precision(self):\n",
        "    print('Precision:', self.precision)\n",
        "\n",
        "  def print_recall(self):\n",
        "    print('Recall:', self.recall)\n",
        "\n",
        "  def print_f1score(self):\n",
        "    print('F1 score:', (2*self.precision*self.recall)/(self.precision+self.recall))\n",
        "\n",
        "  def setValues(self, TP, TN, FP, FN):\n",
        "    self.TP = TP\n",
        "    self.TN = TN\n",
        "    self.FP = FP\n",
        "    self.FN = FN\n",
        "    self.accuracy = (TN+TP)/(TN+TP+FP+FN)\n",
        "    self.precision = TP/(TP+FP)\n",
        "    self.recall = TP/(TP+FN)\n",
        "\n",
        "  def print_all(self):\n",
        "    self.print_accuracy()\n",
        "    self.print_precision()\n",
        "    self.print_recall()\n",
        "    self.print_f1score()\n",
        "\n",
        "\n",
        "#Type in the values for each variable\n",
        "TruePositive = 100 #black in pic\n",
        "TrueNegative = 50 #green in pic\n",
        "FalsePositive = 10 #yellow in pic\n",
        "FalseNegative = 5 #red in pic\n",
        "\n",
        "conf_mat = conf_matrix(TruePositive, TrueNegative, FalsePositive, FalseNegative)\n",
        "\n",
        "#If need to change values in confusion matrix\n",
        "conf_mat.setValues(TruePositive, TrueNegative, FalsePositive, FalseNegative)\n",
        "conf_mat.print_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM9EzGXRjJ6J",
        "outputId": "094b6db4-0ea7-49a7-cd8a-3b10023a21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.9523809523809523\n",
            "F1 score: 0.9302325581395349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions from Quiz 1\n",
        "\n",
        "Compared to classical machine learning, Deep Learning often requires:\n",
        "* few training data and few computations.\n",
        "* few training data and lots of computations.\n",
        "* lots of training data and few computations.\n",
        "* **lots of training data and lots of computations.** (correct)\n",
        "\n"
      ],
      "metadata": {
        "id": "iCHSnpwXu0Do"
      }
    }
  ]
}